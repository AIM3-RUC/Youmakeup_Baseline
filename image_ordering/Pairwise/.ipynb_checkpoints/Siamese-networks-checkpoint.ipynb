{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair-wise image order Learning with Siamese Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import PIL.ImageOps    \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img,text=None,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()    \n",
    "\n",
    "def show_plot(iteration,loss):\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    imageFiles_path = {}\n",
    "    imageFiles_path['train'] = \"../shared_data/train_images/\"\n",
    "    imageFiles_path['test'] =  \"../shared_data/val_images/\"\n",
    "    #train/test split info\n",
    "    movie_names = {}        \n",
    "    movie_names['train'] = np.load(\"../shared_data/train_vids.npy\")\n",
    "    movie_names['test'] =  np.load(\"../shared_data/test_vids.npy\")\n",
    "    \n",
    "    train_batch_size = 32\n",
    "    test_batch_size = 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "input:\n",
    "- video2cnames = {\n",
    "                    '-9GYpCvGIgM':[clip_name1, clip_name2, …], \n",
    "                    ……\n",
    "                  }\n",
    "- gap: the interval step num between image pairs\n",
    "\n",
    "output:\n",
    "- data: save img pairs = [img1_path, img2_path, order]\n",
    "'''\n",
    "def get_train_pairs_gap_n(video2clips,path,gap):\n",
    "    data = []\n",
    "    for key in video2clips:\n",
    "        clip_names = video2clips[key]\n",
    "        cnum = len(clip_names )\n",
    "        start_frames = np.array([int(s.split('_')[-2]) for s in clip_names])\n",
    "        index = start_frames.argsort() \n",
    "        for i in range(cnum - gap):\n",
    "            img1_file = clip_names[index[i]]\n",
    "            img2_file = clip_names[index[i+gap]]\n",
    "            img1_names = sorted(os.listdir(path+img1_file))\n",
    "            img2_names = sorted(os.listdir(path+img2_file))\n",
    "           \n",
    "            leng = len(img1_names)\n",
    "            for k in range(leng-1,leng):#each clip only use one image\n",
    "                order = 0\n",
    "                name1 = img1_names[k]\n",
    "                name2 = img2_names[k]\n",
    "                img1_path = img1_file + '/' + name1\n",
    "                img2_path = img2_file + '/' + name2\n",
    "                #we need to make sure approx 50% of images order is 1 (image i > image j)\n",
    "                #the rest 50% of images order is 0(image i < image j)\n",
    "                should_get_first_order = random.randint(0,1) \n",
    "                if should_get_first_order == 1:\n",
    "                    order = 1\n",
    "                    tmp = img1_path\n",
    "                    img1_path = img2_path\n",
    "                    img2_path = tmp\n",
    "                data.append([img1_path,img2_path,order])\n",
    "    return data \n",
    "\n",
    "def get_train_pairs_gap_more_than_n(video2clips,path,gap):\n",
    "    data = []\n",
    "    for key in video2clips:\n",
    "        clip_names = video2clips[key]\n",
    "        cnum = len(clip_names )\n",
    "        start_frames = np.array([int(s.split('_')[-2]) for s in clip_names])\n",
    "        index = start_frames.argsort() \n",
    "        for i in range(cnum - gap):\n",
    "            img1_file = clip_names[index[i]]\n",
    "            img2_file = []\n",
    "            for j in range(gap,len(index)-i):\n",
    "                img2_file = clip_names[index[i+j]]\n",
    "                img1_names = sorted(os.listdir(path+img1_file))\n",
    "                img2_names = sorted(os.listdir(path+img2_file))\n",
    "                \n",
    "                leng = len(img1_names)\n",
    "                \n",
    "                for k in range(leng-1,leng):#each clip only use one image\n",
    "                    order = 0\n",
    "                    name1 = img1_names[k]\n",
    "                    name2 = img2_names[k]\n",
    "                    img1_path = img1_file + '/' + name1\n",
    "                    img2_path = img2_file + '/' + name2\n",
    "                    #we need to make sure approx 50% of images order is 1 (image i > image j)\n",
    "                    #the rest 50% of images order is 0(image i < image j)\n",
    "                    should_get_first_order = random.randint(0,1) \n",
    "                    if should_get_first_order == 1:\n",
    "                        order = 1\n",
    "                        tmp = img1_path\n",
    "                        img1_path = img2_path\n",
    "                        img2_path = tmp\n",
    "                    data.append([img1_path,img2_path,order])\n",
    "    return data\n",
    "\n",
    "def Extract_splitFiles(files,split,movie_names):\n",
    "    split_image_files = []\n",
    "    split_movie_names = movie_names\n",
    "    for fname in files:\n",
    "        #fname: ETqlg-6bflc_3600_4125\n",
    "        vname = fname[0:11]\n",
    "        if vname in split_movie_names:\n",
    "            split_image_files.append(fname)\n",
    "    return split_image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self,path,split,gap,transform=None):\n",
    "        super(SiameseNetworkDataset, self).__init__()  \n",
    "        self.path = path\n",
    "        self.split = split\n",
    "        image_files = os.listdir(self.path)\n",
    "        self.movie_names = Config.movie_names[split]\n",
    "        self.split_image_files = Extract_splitFiles(image_files,self.split,self.movie_names)\n",
    "        self.clip_num = len(self.split_image_files)\n",
    "        self.pairs = []\n",
    "        self.video2clips = {}\n",
    "        for vname in self.movie_names:\n",
    "            self.video2clips[vname] = []\n",
    "        \n",
    "        for cname in self.split_image_files:\n",
    "            vname = cname[0:11]\n",
    "            self.video2clips[vname].append(cname)\n",
    "        \n",
    "        \n",
    "        for key in self.video2clips:\n",
    "            if len(self.video2clips[key]) < 2:\n",
    "                #print(key,\" only has {} steps\".format(len(self.video2clips[key])))\n",
    "                self.split_image_files.remove(self.video2clips[key][0])\n",
    "        \n",
    "        self.gap = gap\n",
    "        for g in gap:\n",
    "            if g < 5:\n",
    "                pairs = get_train_pairs_gap_n(self.video2clips,path,g)\n",
    "                self.pairs.extend(pairs)\n",
    "            else:\n",
    "                pairs = get_train_pairs_gap_more_than_n(self.video2clips,path,g)\n",
    "                self.pairs.extend(pairs)\n",
    "        random.shuffle(self.pairs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img1_path,img2_path,order = self.pairs[index]\n",
    "        img0 = torch.load(self.path + img1_path)\n",
    "        img1 = torch.load(self.path + img2_path) \n",
    "        return img0, img1, int(order)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gap = [1,2,3,4,5]\n",
    "siamese_trainDataset = SiameseNetworkDataset(\n",
    "        path=Config.imageFiles_path['train'],\n",
    "        split='train',\n",
    "        gap = gap\n",
    ")\n",
    "\n",
    "siamese_testDataset  = SiameseNetworkDataset(\n",
    "        path=Config.imageFiles_path['test'],\n",
    "        split='test',\n",
    "        gap = gap\n",
    ")\n",
    "print\"trainset data size:\",len(siamese_trainDataset)\n",
    "print\"testset data size:\",len(siamese_testDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising some of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsiamese_testDataset  = SiameseNetworkDataset(\n",
    "        path=Config.imageFiles_path['test'],\n",
    "        split='test',\n",
    "        gap = gap\n",
    ")\n",
    "\n",
    "vis_dataloader = DataLoader(vsiamese_testDataset,\n",
    "                        shuffle=True,\n",
    "                        num_workers=0,\n",
    "                        batch_size=8)\n",
    "dataiter = iter(vis_dataloader)\n",
    "\n",
    "\n",
    "example_batch = next(dataiter)\n",
    "concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n",
    "imshow(torchvision.utils.make_grid(concatenated))\n",
    "print \"label:\",example_batch[2].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Net Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        resnet18 = torchvision.models.resnet18(pretrained = True)\n",
    "        self.cnn1 = resnet18\n",
    "        num_ftrs = resnet18.fc.out_features\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(num_ftrs*2, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 2)\n",
    "        )\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        output = self.cnn1(x)   \n",
    "        output = output.view(output.size()[0], -1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        #concate output1&output2\n",
    "        output = torch.cat((output1, output2), dim=1)\n",
    "        output = self.fc1(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    logging.info('Save checkpoint fo {}'.format(filename))\n",
    "    \n",
    "def mkdir(d):\n",
    "    if not os.path.isdir(d) and not os.path.exists(d):\n",
    "        os.system('mkdir -p {}'.format(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(siamese_trainDataset,\n",
    "                        shuffle=True,\n",
    "                        num_workers=0,\n",
    "                        pin_memory = True,\n",
    "                        batch_size=Config.train_batch_size)\n",
    "test_dataloader = DataLoader(siamese_testDataset,\n",
    "                             num_workers=0,\n",
    "                             shuffle=True,\n",
    "                             pin_memory = True,\n",
    "                             batch_size=Config.test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model parameters set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SiameseNetwork().cuda()\n",
    "#net = SiameseNetwork()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "base_lr = 0.0001\n",
    "step_size = 5\n",
    "Config.train_number_epochs = 20\n",
    "optimizer = optim.SGD(net.parameters(), lr=base_lr, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"./contrastive_loss/\"\n",
    "snapshot = \"./checkpoints/\"\n",
    "mkdir(snapshot)\n",
    "mkdir(log_file)\n",
    "counter = []\n",
    "loss_history = [] \n",
    "iteration_number = 0\n",
    "init_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = base_lr\n",
    "lr_iter = 0\n",
    "for epoch in range(init_epoch,Config.train_number_epochs):\n",
    "    scheduler.step()\n",
    "    lr_iter += 1\n",
    "    #train for one epoch-----------------------------------------------------------------------------------\n",
    "    net.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    i = 0\n",
    "    with tqdm(train_dataloader, position=0, leave=True) as t:\n",
    "        for data in t:\n",
    "            img0, img1 , label = data\n",
    "            img0, img1 , label = img0.cuda(), img1.cuda(), label.cuda()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            output= net(img0,img1)\n",
    "            loss_contrastive = criterion(output,label)\n",
    "        \n",
    "            epoch_loss += loss_contrastive[0].item()\n",
    "            pred = torch.max(output, 1)[1]\n",
    "            epoch_correct = (pred == label).sum()\n",
    "            epoch_acc += epoch_correct[0].item()\n",
    "        \n",
    "            loss_contrastive.backward()\n",
    "            optimizer.step()\n",
    "            iters = len(siamese_trainDataset)/Config.train_batch_size\n",
    "            if i % int(iters/10) == 0 :\n",
    "                iteration_number +=iters\n",
    "                counter.append(iteration_number)\n",
    "                loss_history.append(loss_contrastive.item())\n",
    "            i += 1  \n",
    "        if lr_iter%step_size == 0:\n",
    "            lr = lr*0.1\n",
    "    t.close()    \n",
    "    print('Epoch: {}, Train Loss: {:.6f}, Train Acc: {:.6f}'.format(epoch, epoch_loss / (len(siamese_trainDataset)), epoch_acc / (len(siamese_trainDataset))))     \n",
    "    \n",
    "    #evaluate for evach epoch----------------------------------------------------------------------------------\n",
    "    net.eval()\n",
    "    eval_acc = 0.0\n",
    "    eval_loss = 0.0\n",
    "    i = 0\n",
    "    with tqdm(test_dataloader, position=0, leave=True) as test:\n",
    "        for data in test:\n",
    "            img0, img1, label = data\n",
    "            img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()\n",
    "            out = net(img0,img1)\n",
    "            loss_contrastive = criterion(out,label)\n",
    "            \n",
    "            pred = torch.max(out, 1)[1]\n",
    "            num_correct = (pred == label).sum()\n",
    "            eval_acc += num_correct[0].item()\n",
    "            eval_loss += loss_contrastive[0].item()\n",
    "            i += 1\n",
    "    test.close()\n",
    "    print('Epoch: {}, Test Loss: {:.6f}, Test Acc: {:.6f}'.format(epoch,eval_loss/len(siamese_testDataset),eval_acc/len(siamese_testDataset)))\n",
    "    #write to logfile\n",
    "    logfile=open(log_file + 'contrastive_gap{}.txt'.format(gap),'a')\n",
    "    logfile.write( 'Epoch: {},lr: {:.6f}, Train Loss: {:.6f}, Train Acc: {:.6f}, Test Loss: {:.6f}, Test Acc: {:.6f}\\n'.format(\n",
    "        epoch, lr, epoch_loss / len(siamese_trainDataset), epoch_acc / len(siamese_trainDataset),\n",
    "        eval_loss/len(siamese_testDataset),eval_acc/len(siamese_testDataset)))\n",
    "    logfile.close()\n",
    "    \n",
    "    #save model paramers for each epoch\n",
    "    filename = snapshot+'checkpoint_gap{}_epoch{}.pth.tar'.format(gap,epoch)\n",
    "    save_checkpoint(\n",
    "        {\n",
    "            'epoch':epoch,\n",
    "            'lr':lr,\n",
    "            'gap':gap,\n",
    "            'state_dict':net.state_dict()\n",
    "        },\n",
    "        filename\n",
    "    )\n",
    "#show_plot(counter,loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluation(test_dataloader,model,siamese_testDataset):\n",
    "    model.eval()\n",
    "    eval_acc = 0.0\n",
    "    for data in tqdm(test_dataloader):\n",
    "        img0, img1, label = data\n",
    "        img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()\n",
    "        with torch.no_grad():\n",
    "            out = model(img0,img1)\n",
    "            pred = torch.max(out, 1)[1]\n",
    "            num_correct = (pred == label).sum()\n",
    "            eval_acc += num_correct[0].item()\n",
    "    eval_acc =  eval_acc/len(siamese_testDataset)\n",
    "    print('epoch:{}  test acc: {:.6f}'.format(epoch,eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint_path = './pretrain_model/pairwise_with_CL.pth.tar' #Set your chenckpoint path here\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model = SiameseNetwork().cuda()\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "siamese_testDataset  = SiameseNetworkDataset(\n",
    "        path=Config.imageFiles_path['test'],\n",
    "        split='test',\n",
    "        gap = [1,2,3,4,5]\n",
    ")\n",
    "print(len(siamese_testDataset))\n",
    "test_dataloader = DataLoader(siamese_testDataset,num_workers=0,shuffle=True,pin_memory = True,batch_size=Config.test_batch_size)\n",
    "Evaluation(test_dataloader,model,siamese_testDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate choice accuracy on image ordering task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Set your chenckpoint path here\n",
    "'''\n",
    "set_checkpoint_path = './pretrain_model/pairwise_with_CL.pth.tar' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Validation_Info(json_path):\n",
    "    qinfos = []\n",
    "    with open(json_path, \"r\") as file:\n",
    "        for index,f in enumerate(file):\n",
    "            line  = json.loads(f)\n",
    "            question_info = {}\n",
    "            \n",
    "            question_info['question_id'] = line['question_id']# number: 1 (related image dir name: 1/)\n",
    "            question_info['video_id']  = line['video_id']#\"-9GYpCvGIgM\"\n",
    "            question_info['step_caption'] = line['step_caption']#list of step captions\n",
    "            question_info['groundtruth'] = line['groundtruth'] #[4, 1, 2, 5, 3]\n",
    "            question_info['candidate_answer'] = line['candidate_answer']\n",
    "            qinfos.append(question_info)\n",
    "    return qinfos  \n",
    "\n",
    "def Get_QueryImages(imgs_path):\n",
    "    imgs = []\n",
    "    transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(224),\n",
    "            torchvision.transforms.CenterCrop(224),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                             [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    img_name = sorted(os.listdir(imgs_path))\n",
    "    for n in img_name:\n",
    "        if n[-4:] != '.jpg':\n",
    "            continue\n",
    "        img_path = imgs_path + n\n",
    "        with open(img_path, 'rb') as f:\n",
    "            img = PIL.Image.open(f)\n",
    "            img = img.convert('RGB') \n",
    "            img = transform(img)\n",
    "            imgs.append(img)\n",
    "    return imgs\n",
    "\n",
    "def if_same(select_answer,ground_truth):\n",
    "    count = 0\n",
    "    for i in range(len(select_answer)):\n",
    "        count += (select_answer[i] == ground_truth[i])\n",
    "    return int(count == 5)\n",
    "\n",
    "def json_data_save(path, data):\n",
    "    jsdata = json.dumps(data)\n",
    "    jsfile = open(path, 'w')\n",
    "    jsfile.write(jsdata)\n",
    "    jsfile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import json\n",
    "from Levenshtein import *\n",
    "\n",
    "img_path = {}\n",
    "img_path['valid'] = \"../../../YouMakeup/data/task/image_ordering/valid/images/\"\n",
    "img_path['test'] = \"../../../YouMakeup/data/task/image_ordering/test/images/\"\n",
    "\n",
    "question_path = {}\n",
    "question_path['valid'] = \"../../../YouMakeup/data/task/image_ordering/valid/image_ordering_validation.json\"\n",
    "question_path['test'] = \"../../../YouMakeup/data/task/image_ordering/test/image_ordering_test.json\"\n",
    "\n",
    "init_pic = {}\n",
    "final_answer = {}\n",
    "init_pic_acc = {}\n",
    "choice_acc = {}\n",
    "for split in  [\"valid\",\"test\"]:\n",
    "    images_path = img_path[split]\n",
    "    questions_info = Get_Validation_Info(question_path[split])\n",
    "    correct_choice_num = 0.0\n",
    "    init_pic_correct_num = 0.0\n",
    "    init_pic[split] = []\n",
    "    select_answer = []\n",
    "    final_answer[split] = {} #save predict answer\n",
    "    \n",
    "    #load model\n",
    "    model = SiameseNetwork().cuda()\n",
    "    checkpoint_path =  set_checkpoint_path \n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    for i,query in enumerate(questions_info):\n",
    "        #get five query imgs\n",
    "        img_dir = str(query['question_id'])\n",
    "        imgs_path = images_path + img_dir + '/'\n",
    "        imgs = Get_QueryImages(imgs_path)\n",
    "        #1-2，1-3，1-4，1-5，2-3，2-4，2-5，3-4，3-5，4-5 pairs comparasion\n",
    "        imgs_list1 = [imgs[0],imgs[0],imgs[0],imgs[0],imgs[1],imgs[1],imgs[1],imgs[2],imgs[2],imgs[3]]\n",
    "        imgs_list2 = [imgs[1],imgs[2],imgs[3],imgs[4],imgs[2],imgs[3],imgs[4],imgs[3],imgs[4],imgs[4]]\n",
    "        imgs_list1 = torch.stack(imgs_list1).float()\n",
    "        imgs_list1 = torch.autograd.Variable(imgs_list1).cuda()\n",
    "        imgs_list2 = torch.stack(imgs_list2).float()\n",
    "        imgs_list2 = torch.autograd.Variable(imgs_list2).cuda()\n",
    "        pred = None\n",
    "        predc = None\n",
    "        with torch.no_grad():\n",
    "            out = F.softmax(model(imgs_list1,imgs_list2))\n",
    "            pred = np.array(torch.max(out, 1)[1])\n",
    "            predc = np.array(out[:,1])\n",
    " \n",
    "        ground_truth = query['groundtruth']\n",
    "        print \"--------{}   query {} -------\".format(split,i+1)\n",
    "        print \"ground truth:\",ground_truth\n",
    "        \n",
    "        #choose answer algorithm\n",
    "        pred_dict = {'12':predc[0],'13':predc[1],'14':predc[2],'15':predc[3],'23':predc[4],'24':predc[5],'25':predc[6],'34':predc[7],'35':predc[8],'45':predc[9],\n",
    "                 '21':1-predc[0],'31':1-predc[1],'41':1-predc[2],'51':1-predc[3],'32':1-predc[4],'42':1-predc[5],'52':1-predc[6],'43':1-predc[7],'53':1-predc[8],'54':1-predc[9]}\n",
    "        max_score = -1\n",
    "        confidence = {}\n",
    "        for answer in query['candidate_answer']:\n",
    "            score = 0\n",
    "            pairs = []\n",
    "            for j in range(4):\n",
    "                for k in range(j+1,5):   \n",
    "                    pairs.append(str(answer[k])+str(answer[j]))\n",
    "            for p in pairs:\n",
    "                score += pred_dict[p]\n",
    "            confidence[str(answer)] = score\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                select_answer = answer\n",
    "        count = if_same(select_answer,ground_truth)\n",
    "        correct_choice_num += count\n",
    "        print \"choose answer:\",select_answer, bool(count)\n",
    "        final_answer[split][query['question_id']] = select_answer\n",
    "\n",
    "    \n",
    "        #select init image which has the highest average pairwise comparison probability\n",
    "        pick_init_pic = -1\n",
    "        max_prob = -1\n",
    "        for img_id in ['1','2','3','4','5']:\n",
    "            prob = 0.0\n",
    "            for rest_id in ['1','2','3','4','5']:\n",
    "                if rest_id != img_id:\n",
    "                    prob += pred_dict[rest_id+img_id]\n",
    "            prob /= 4.0\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                pick_init_pic = int(img_id)\n",
    "        init_pic_correct_num += int(ground_truth[0] == pick_init_pic)\n",
    "        init_pic[split].append(pick_init_pic)\n",
    "        \n",
    "        #calculate choice acc and init pic selec acc\n",
    "        init_pic_acc[split] = init_pic_correct_num/len(questions_info)\n",
    "        choice_acc[split] = correct_choice_num/len(questions_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"valid\",\"test\"]:\n",
    "    print \"----------------------{}-----------------------\".format(split)\n",
    "    np.save(\"init_pics_{}_{:.3f}.npy\".format(split,init_pic_acc[split]),init_pic[split])\n",
    "    #save predicted answers to .json\n",
    "    path = \"pairwise_{}_answer{:.3f}.json\".format(split,choice_acc[split])\n",
    "    json_data_save(path, final_answer)\n",
    "    print \"Choice accuracy: {:.3f}\".format(choice_acc[split])\n",
    "    print \"Choosed answers have been save in : pairwise_{}_answer{:.3f}.json\".format(split,choice_acc[split])\n",
    "    print \"Init images have been save in     : init_pics_{}_{:.3f}.npy\".format(split,init_pic_acc[split])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "python2.7",
   "language": "python",
   "name": "python27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
